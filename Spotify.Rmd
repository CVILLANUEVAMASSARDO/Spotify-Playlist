---
title: "Spotify - Recomendación de Playlist"
subtitle: "Universidad Adolfo Ibáñez"
author: "Cristián Daniel Villanueva Massardo"
date: "27 de mayo de 2023"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(output = "README.md", echo = TRUE)
```

# 1. Introducción

### 1.1 Análisis de música
El objetivo de esta tarea es ver cómo abordan un problema de la vida real, con incertidumbre, ya que en el mundo profesional se enfrentarán con tareas similares.

### 1.2 Descripción del problema
Spotify recomienda nuevas canciones a sus usuarios basándose en las reproducciones pasadas y en estilos musicales similares. Esto lo hace a través de diversos algoritmos que relacionan las canciones a través de diferentes atributos como la verbosidad o energía. Una lista con las mediciones que se hace para cada canción esta disponible en la documentación de la API de Spotify(https://developer.spotify.com/documentation/web-api/reference/).

El objetivo principal de este encargo es crear un programa computacional que permita crear una lista de reproducción de 3 horas de duración basándose en alguna canción de referencia. La base de datos incluye 447.622 canciones, con 36 de las variables descritas en la documentación de la API.

Como resultado de la prueba, genere un reporte en RMarkdown describiendo las etapas de su proceso, los modelos de clustering utilizados, los resultados obtenidos y el código empleado. Debe explicar cómo limpió los datos, como se eligieron y generaron las variables, y como construyó su lógica.

### 1.3 Metodología utilizada
Se estudiarán las características de audio de las distintas canciones con el objetivo de realizar análisis de agrupaciones para ofrecer recomendaciones que sean similares a la canción de referencia utilizada. Se intentará comprobar si existe algún patrón para clusterizar y en qué se diferencia cada cluster.

# 2. Paquetes requeridos

```{r, echo = TRUE, message = FALSE}
library(tidyverse)
library(corrplot)
library(factoextra)
```

### 2.1 Información de paquetes utilizados

**tidyverse** - Permite la manipulación, importación, exploración y visualización de datos. Contiene los paquetes readr, dplyr, ggplot2, tibble, tidyr, purr, stringr y forcats.

**corrplot** - Visualización de correlación entre variables.

# 3. Preparación de los datos

### 3.1 Fuente
Se utilizará el archivo "beats.RData" que contiene 447.622 canciones con 36 variables cada una.

Datos extraídos de la API de Spotify.

```{r}
load('beats.RData')
beats <- beats
```

```{r, echo = FALSE, message = TRUE}
cat("El dataframe contiene", nrow(beats), "filas y", ncol(beats), "columnas.")
```

### 3.2 Información
Nombre de columna, tipo de dato y registro de la columna.

```{r}
glimpse(beats)
```

### 3.2 Limpieza

#### 3.2.1 Removiendo duplicados
Se remueven las canciones duplicadas.

```{r}
beats <- beats[!duplicated(beats$track_id),]
```

#### 3.2.2 Creando variables
Se crea la variable que mide la duración de la canción en minutos.

```{r}
beats$duration_min <- beats$duration_ms / 60000
```

#### 3.2.3 Removiendo variables
Para el análisis que se busca, se necesitan las características de sonido de la canción, su duración y su nombre.

Solo se conservaran estas variables y se removerá el resto.

```{r}
beats <- beats %>% select(c(track_name,duration_min,8:18))
```

#### 3.2.4 Removiendo NA's
Se observa que las variables seleccionadas están limpias y no presentan NA´s.

No es necesario realizar limpieza.

```{r}
colSums(is.na(beats))
```

#### 3.2.5 Tamaño final del archivo

```{r, echo = FALSE, message = TRUE}
cat("El dataframe contiene", nrow(beats), "filas y", ncol(beats), "columnas.")
```

### 3.3 Descripción de los atributos
Cada canción presente en la base de datos contiene los siguientes atributos:

**track_name** - Nombre de la canción.

**duration_min** - Duración en minutos.

**danceability** - Qué tan bailable es la canción, donde 0,0 es el menor y 1,0 es el mayor.

**energy** - Que tanta intensidad y actividad representa, donde 0,0 es el menor y 1,0 es el mayor.

**key** - Clave de la pista. 0 es Do, 1 es Do#, 2 es Re y así sucesivamente. Cuando no presenta tonalidad es -1.

**loudness** - Qué tanto decibelios (dB) tiene la canción. Oscila entre -60 y 0 dB.

**mode** - Contenido melódico de la canción. Mayor es 1 y Menor es 0.

**speechiness** - Presencia de palabras habladas. 1 es el mayor y 0 es el menor.

**acousticness** - Acústica de la canción. 1 es acústica y 0 es no acústica.

**instrumentalness** - Presencia de voces. 1 es instrumento y 0 es voz.

**liveness** - Detecta público en la grabación. Mayor valor es alta presencia de público.

**valence** - Positividad de la canción. 1 es alegre y 0 es triste.

**tempo** - Pulsaciones por minuto (BPM).

# 4 Análisis exploratorio de datos (EDA)

### 4.1 Mapa de correlación
Se utiliza el coeficiente de correlación de Pearson, que mide la correlación lineal entre dos variables continuas que varía entre -1 y 1, donde 1 indica una correlación positiva perfecta y -1 indica una correlación negativa perfecta, siendo 0 la falta de correlación lineal.

$$\rho = \frac{\text{cov}(X,Y)}{\sigma_x \sigma_y}$$

En el gráfico se visualiza que las variables que se utilizaran en el gráfico están fuertemente correlacionadas, a excepción de key y mode, que son valores enteros y no se encuentra una relación ni positiva ni negativa con el resto de variables.

En cambio, tomando como ejemplo la variable danceability. Esta se relaciona positivamente con loudness y valence. Resultado lógico por lo que representa cada variable, esperando que una canción con alto dB y positividad genere un ambiente de deseos de bailar.

La variable energy presenta una correlación positiva con loudness y liveness, mientras que una correlación débil con acousticness. Esto porque las canciones con alta intensidad presentan un sonido más fuerte (dB), donde también podría contener público en la grabación. La variable acousticness se encuentra en canciones más tranquilas y sin tanta intensidad, por lo que es lógico que tendrá una correlación débil.



```{r}
beats_atributos <- select(beats, 3:13)
corrplot(cor(beats_atributos))
```

### 4.2 Histograma
Se visualiza la forma de distribución, el rango y la concentración de los datos.

Se identifican concentraciones importantes en las variables, infiriendo que existen subgrupos dentro de los datos. Esto podría reflejarse en la presencia de categorías o generos de músicas.

```{r}
beats_grafico <- beats_atributos %>% tidyr::gather(key = "variable", value = "valor")

ggplot(beats_grafico, aes(x = valor, fill = variable)) +
  geom_histogram(binwidth=0.25) +
  facet_wrap(~ variable, scales = "free") +
  scale_fill_hue() +
  guides(fill = "none") +
  theme(legend.position = "none") +
  labs(x = "Valor", y = "Frecuencia") +
  theme_minimal()
```

### 4.3 Boxplot
Resumen estadísticos de los datos, identificando la medianta, cuartiles y valores atípicos.Permite identificar la simetría de la distribución y la variabilidad.

Se identifican datos atípicos en los valores extremos que pueden tener un impacto significativo en el análisis de datos.

Para dar una explicación a los datos atípicos que se visualizan, se necesita una nueva variable llamada "genero" que podría ser la explicación de la concentración de datos en ciertos valores.

También es necesario conocer la variabilidad y dispersión de la variable "genero", ya que la muestra de datos podría concentrarse en solo un pequeño grupo de generos y el resto de canciones que pertenecen a un genero distinto representen los datos atípicos en las variables analizadas.

```{r}
ggplot(beats_grafico, aes(x = variable, y = valor)) +
  geom_boxplot(aes(fill = variable)) +
  facet_wrap(~ variable, scales = "free") +
  scale_fill_hue() +
  guides(fill = "none") +
  theme(legend.position = "none") +
  labs(x = "Variable", y = "Valor") +
  theme_minimal()
```

# 5 Construcción del modelo

### 5.1 Escalar datos
Debido a que cada variable tiene su escala única y no pueden medirse de la misma manera, se deberán escalar los datos.

Se realizará una estandarización que dejará la media = 0 y la desviación estandar = 1 para todas las variables.

$$X_{scaled} = \frac{(x - \bar{x})^2}{\sigma}$$

```{r}
summary(beats_atributos)
```

```{r}
beats_escalar <- scale(beats_atributos)
summary(beats_escalar)
```

### 5.2 Método del codo (Elbow)
Se utiliza el método del codo, o en inglés, Elbow method, para realizar un análisis de clusters (agrupamiento) para determinar el número óptimo de grupos en un conjunto de datos. 

El objetivo es encontrar el punto en el que la adición de más clusters no proporcione una mejora significativa en la calidad del agrupamiento. 

Esto se define como la suma de errores cuadráticos (SSE), que es la suma de las distancias al cuadrado de cada punto de datos dentro de su cluster correspondiente. Esto mide la variación total dentro de los clusters.

$$SSE = \sum (y - \hat{y})^2$$

Para identificar el "codo", se busca que la adición de más clusters ya no reduce significativamente el SSE. Esto suele representar un equilibrio entre una buena división de los datos en grupos coherentes y evitar una división excesiva que puede conducir a un sobreajuste.


```{r, warning=FALSE}
wss <- function(data, maxCluster = 30) {
  SSw <- (nrow(data) - 1) * sum(apply(data, 2, var))
  SSw <- vector()
  for (i in 2:maxCluster) {
    SSw[i] <- sum(kmeans(data, centers = i)$withinss)
  }
  plot(1:maxCluster, SSw, type = "o", xlab = "Clusters (K)", ylab = "SSE", pch=19)
}

wss(beats_escalar)
```

```{r}
beats_escalar_kmeans <- kmeans(beats_escalar, centers = 8)
beats_escalar_kmeans$size
```

```{r}
beats_escalar_kmeans$centers
```

```{r}
fviz_cluster(beats_escalar_kmeans, data=beats_escalar)
```

### 5.2 Clusterización K-means

```{r}

```